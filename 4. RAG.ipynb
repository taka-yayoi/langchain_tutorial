{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd94ff9a-07d9-48c3-b6ac-0891ad2edf40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Retrieval Augmented Generation (RAG) アプリを構築する: パート1\n",
    "\n",
    "LLMによって可能になる最も強力なアプリケーションの1つは、洗練された質問応答 (Q&A) チャットボットです。これらは特定のソース情報に関する質問に答えるアプリケーションです。これらのアプリケーションは、リトリーバル強化生成 ([RAG](https://python.langchain.com/docs/concepts/rag/)) として知られる技術を使用します。\n",
    "\n",
    "これは複数パートのチュートリアルです:\n",
    "\n",
    "- [パート1](https://python.langchain.com/docs/tutorials/rag/) (このガイド) では、RAGを紹介し、最小限の実装を説明します。\n",
    "- [パート2](https://python.langchain.com/docs/tutorials/qa_chat_history/)では、会話スタイルのインタラクションやマルチステップのリトリーバルプロセスに対応するために実装を拡張します。\n",
    "\n",
    "このチュートリアルでは、テキストデータソース上でシンプルなQ&Aアプリケーションを構築する方法を示します。途中で、典型的なQ&Aアーキテクチャについて説明し、より高度なQ&A技術のための追加リソースを強調します。また、LangSmithがアプリケーションのトレースと理解にどのように役立つかを見ていきます。アプリケーションが複雑になるにつれて、LangSmithはますます役立つようになります。\n",
    "\n",
    "基本的なリトリーバルにすでに精通している場合は、[さまざまなリトリーバル技術の概要](https://python.langchain.com/docs/concepts/retrieval/)にも興味があるかもしれません。\n",
    "\n",
    "注: ここでは非構造化データのQ&Aに焦点を当てています。構造化データに対するRAGに興味がある場合は、[SQLデータに対する質問応答](https://python.langchain.com/docs/tutorials/sql_qa/)に関するチュートリアルをご覧ください。\n",
    "\n",
    "[Build a Retrieval Augmented Generation \\(RAG\\) App: Part 1 \\| 🦜️🔗 LangChain](https://python.langchain.com/docs/tutorials/rag/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dc878ee-a401-4505-8445-f1ef446423d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 概要\n",
    "\n",
    "典型的なRAGアプリケーションには2つの主要なコンポーネントがあります:\n",
    "\n",
    "- **インデックス作成:** ソースからデータを取り込み、インデックスを作成するパイプライン。*これは通常オフラインで行われます。*\n",
    "- **リトリーバルと生成:** 実際のRAGチェーンで、実行時にユーザーのクエリを受け取り、インデックスから関連データを取得し、それをモデルに渡します。\n",
    "\n",
    "注: このチュートリアルのインデックス作成部分は、[主にセマンティック検索のチュートリアル](https://python.langchain.com/docs/tutorials/retrievers/)に従います。\n",
    "\n",
    "生データから回答までの最も一般的な完全なシーケンスは次のとおりです:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "255bd778-a472-405f-a89c-b28acf88510b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### インデックス作成\n",
    "\n",
    "1. **ロード:** まず、データをロードする必要があります。これは[ドキュメントローダー](https://python.langchain.com/docs/concepts/document_loaders/)を使用して行います。\n",
    "1. **分割:** [テキストスプリッター](https://python.langchain.com/docs/concepts/text_splitters/)は、大きな`Documents`を小さなチャンクに分割します。これは、データのインデックス作成やモデルに渡す際に役立ちます。大きなチャンクは検索が難しく、モデルの有限のコンテキストウィンドウに収まりません。\n",
    "1. **保存:** 分割したデータを保存し、後で検索できるようにインデックスを作成する場所が必要です。これは通常、[ベクトルストア](https://python.langchain.com/docs/concepts/vectorstores/)と[エンベディング](https://python.langchain.com/docs/concepts/embedding_models/)モデルを使用して行います。\n",
    "\n",
    "![](https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c4cc85c-0ee7-477b-8c2b-af5158b76827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### リトリーバルと生成\n",
    "\n",
    "1. **リトリーバル:** ユーザー入力を受け取り、リトリーバーを使用してストレージから関連するスプリットを取得します。\n",
    "1. **生成:** [チャットモデル](https://python.langchain.com/docs/concepts/chat_models/) / [LLM](https://python.langchain.com/docs/concepts/text_llms/)が、質問と取得したデータを含むプロンプトを使用して回答を生成します。\n",
    "\n",
    "![](https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png)\n",
    "\n",
    "データのインデックス作成が完了したら、[LangGraph](https://langchain-ai.github.io/langgraph/)をオーケストレーションフレームワークとして使用し、リトリーバルと生成のステップを実装します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9fbf481-182c-4052-933b-b893c890f9be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e5e7be-c0ab-4702-af09-b52734510edd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph langchain[openai] mlflow bs4\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66240a14-5fc1-43a8-a183-50d8e62bff2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# MLflow Tracingの有効化\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273eb039-af69-4430-bce7-59128c594dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(\"demo-token-takaaki.yayoi\", \"openai_api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cce9c3a6-480e-4259-80cb-8387490e36f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## コンポーネント\n",
    "\n",
    "LangChainの統合スイートから3つのコンポーネントを選択する必要があります。\n",
    "\n",
    "[chat model](https://python.langchain.com/docs/integrations/chat/)を選択します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f22d37b6-42cc-47a4-b081-ae2df650f792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3cb56ef-36a3-4c01-a1ab-1b30fcd9a7e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "[Embeddings model](https://python.langchain.com/docs/integrations/text_embedding/)を選択します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74ca49b5-bdad-4ef5-94ce-cb48b3017b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc2e771b-7ba8-47fb-9512-c11759e14352",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "[Vector store](https://python.langchain.com/docs/integrations/vectorstores/)を選択します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8398e1a6-6c3f-40c5-af46-712a1079ef50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f48a997b-c21e-4d3d-96b9-a7ebb68997ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## プレビュー\n",
    "\n",
    "このガイドでは、ウェブサイトのコンテンツに関する質問に答えるアプリを作成します。使用する特定のウェブサイトは、私による[はじめてのDatabricks](https://qiita.com/taka_yayoi/items/8dc72d083edb879a5e5d)のブログ投稿であり、投稿の内容に関する質問をすることができます。\n",
    "\n",
    "これを約50行のコードでシンプルなインデックス作成パイプラインとRAGチェーンを作成して行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba1b474d-221e-4463-b35f-1c0fdebf4ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# ブログの内容をロードしてチャンクに分割\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://qiita.com/taka_yayoi/items/8dc72d083edb879a5e5d\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"p-items_main\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# チャンクをインデックス化\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# 質問応答のためのプロンプトを定義\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# アプリケーションの状態を定義\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# アプリケーションのステップを定義\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# アプリケーションをコンパイルしてテスト\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68ffc88f-38af-46a0-9cd6-d8fc673504f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9bd3c35-ea91-4d42-af87-3e820dfe9199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dad94b0e-c416-41e4-92f6-6e9a475836c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = graph.invoke({\"question\": \"DatabricksとJupyter Notebookの違いは？\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d6744ed-3c35-4398-957f-eae44d6c6ba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "4. RAG",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
